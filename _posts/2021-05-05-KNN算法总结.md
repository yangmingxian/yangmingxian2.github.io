---
title: KNN算法总结
author: Mingxian Yang
date: 2021-05-05 18:10:00 +0800
categories: [Learning Notes]
tags: [Machine Learning]
---

> K最近邻(k-Nearest Neighbor，KNN)分类算法，是一个理论上比较成熟的方法，也是最简单的机器学习算法之一。该方法的思路是：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。

### KNN介绍
KNN可以说是最简单的分类算法之一，同时，它也是最常用的分类算法之一，注意KNN算法是**有监督学习**中的分类算法，它看起来和另一个机器学习算法Kmeans有点像（Kmeans是无监督学习算法）。

KNN 是一种基本的分类与回归方法，给定一个数据集，在数据集中找出与新实例最近的 k 个实例，选择这个 k 个实例中出现次数最多的类别作为新实例的类别。  

KNN 主要有三个基本的要素：k 值的选择、距离度量、分类决策规则

### KNN实现过程
      计算当前待分类实例与训练数据集中的每个样本实例的距离；
      按照距离递增次序排序；
      选取与待分类实例距离最小的k个训练实例；
      统计这k个实例所属各个类别数；
      将统计的类别数最多的类别作为待分类的预测类别 

### 距离计算
曼哈顿距离和欧氏距离。

### KNN中K的选择
答案是通过交叉验证（将样本数据按照一定比例，拆分出训练用的数据和验证用的数据，比如6：4拆分出部分训练数据和验证数据），从选取一个较小的K值开始，不断增加K的值，然后计算验证集合的方差，最终找到一个比较合适的K值。  

在实际应用中，K 一般选一个比较小的值，然后采用交叉验证法来选择最优的 K 值。

   - 较小的 K 值，相当于用较小领域中的训练实例进行预测，学习的近似误差会减小，但是估计误差会增大。  
   K 值减小，意味着整体模型变得复杂，容易发生过拟合

   - 较大的 K 值，相当于用较大领域中的训练实例进行预测，学习的近似误差会增大，但是估计误差会减小。  
   K 值的增大，意味着整体的模型变得简单
  


### KNN的优缺点
优点  
思想简单，理论成熟，既可以做分类也可以做回归  
训练时间复杂度为 O(n)  
准确度高，对数据没有假设    

缺点  
计算量大  
样本不平衡问题  
需要大量的内存  

### KNN和K—Means的比较  
K-Means 是聚类算法，KNN 是分类算法。  
KNN需要标记点，因此是有监督的学习，而k-means不是，因此是无监督学习。  
K均值聚类仅需要一组未标记的点和阈值：算法将采用未标记的点并逐渐学习如何通过计算不同点之间的距离的平均值将它们聚类成组。  
最后，K 值的含义不同。K-Means 中的 K 值代表 K 类。KNN 中的 K 值代表 K 个最接近的邻居。
