---
title: K-Means算法总结
author: Mingxian Yang
date: 2021-05-05 18:10:00 +0800
description: k-means algorithm算法是一个聚类算法，把n的对象根据他们的属性分为k个分割(k < n)。它与处理混合正态分布的最大期望算法很相似，因为他们都试图找到数据中自然聚类的中心。它假设对象属性来自于空间向量，并且目标是使各个群组内部的均方误差总和最小。
categories: [机器学习]
tags: [机器学习]
render_with_liquid: false
math: true
---



### 实现过程
```
选取k个点作为初始质心
repeat
   将每个点指派到最近的质心，形成k个簇
   重新计算每个簇的质心
until
   簇不再发生变化或者达到了指定的迭代次数
```


```
创建k个点作为初始质心（可以随机选择）
   当当前任意一个簇的分配结果发生改变时：
      对数据集中的每个数据点：
         对每一个质心：
            计算质心和数据点之间的距离
         将数据点分配到距离最近的簇
      对每一个簇：
         求出均值并将其更新为质心
```

### 数据到中心的距离计算
曼哈顿距离：$d_{12}=\left|x_{1}-x_{2}\right|+\left|y_{1}-y_{2}\right|$  

欧几里得距离：$d_{12}=\sqrt{\left(x_{1}-x_{2}\right)^{2}+\left(y_{1}-y_{2}\right)^{2}}$  

### 初始K个点的选择
值得注意的是，初始点的选择不同可能会影响到分类结果。  
- 这k个点的距离尽可能远
- 可以对数据先进行层次聚类（博客后期会更新这类聚类算法），得到K个簇之后，从每个类簇中选择一个点，该点可以是该类簇的中心点，或者是距离类簇中心点最近的那个点。
  
### K值的选取
可以采用手肘法，其核心指标是误差平方和（SSE，sum of the squared errors）  
$$S S E=\sum_{i=1}^{k} \sum_{p \in C_{i}}\left|p-m_{i}\right|^{2}$$ 

其中，$C_{i}$是第i个簇，$p$是$C_{i}$中的样本点，$m_i$是$C_{i}$的质心（$C_{i}$中所有样本的均值），SSE是所有样本的聚类误差，代表了聚类效果的好坏。
手肘法的核心思想是：随着聚类数k的增大，样本划分会更加精细，每个簇的聚合程度会逐渐提高，那么误差平方和SSE自然会逐渐变小。并且，当k小于真实聚类数时，由于k的增大会大幅增加每个簇的聚合程度，故SSE的下降幅度会很大，而当k到达真实聚类数时，再增加k所得到的聚合程度回报会迅速变小，所以SSE的下降幅度会骤减，然后随着k值的继续增大而趋于平缓，也就是说SSE和k的关系图是一个手肘的形状，而这个肘部对应的k值就是数据的真实聚类数。当然，这也是该方法被称为手肘法的原因。

### 如何快速收敛数据量很大的K-Means
Mini Batch Kmeans使用了一种叫做Mini Batch（分批处理）的方法对数据点之间的距离进行计算。Mini Batch的好处是计算过程中不必使用所有的数据样本，而是从不同类别的样本中抽取一部分样本来代表各自类型进行计算。由于计算样本数量少，所以会相应的减少运行时间，但另一方面抽样页必然会带来准确度的下降。  

该算法的迭代步骤有两步：  
1）从数据集中随机抽取一些数据形成小批量，把他们分配给最近的质心  
2）更新质心：与k均值算法相比，数据的更新是在每一个小的样本集上。对于每一个小批量，通过计算平均值得到更新质心，并把小批量里的数据分配给该质心，随着迭代次数的增加，这些质心的变化是逐渐减小的，直到质心稳定或者达到指定的迭代次数，停止计算。

### K-Means的优缺点
K-Means的主要优点：  
（1）原理简单，容易实现  
（2）可解释度较强  

K-Means的主要缺点：  
（1）K值很难确定  
（2）局部最优  
（3）对噪音和异常点敏感  
（4）需样本存在均值（限定数据种类）  
（5）聚类效果依赖于聚类中心的初始化  
（6）对于非凸数据集或类别规模差异太大的数据效果不好  
