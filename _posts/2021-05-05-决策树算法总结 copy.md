---
title: 决策树算法总结
author: Mingxian Yang
date: 2021-05-05 18:10:00 +0800
description: 决策树（Decision Tree）算法，属于机器学习有监督分类算法的一种，决策树是一个预测模型。决策树是一种用于对实例进行分类的树形结构。由结点（node）和有向边（directed edge）组成。结点有两种类型：内部结点与叶结点，内结点表示一个特征或属性的测试条件（用于分开具有不同特性的记录），叶结点表示一个分类。使用决策树模型，首先构建决策树，然后从决策树的根结点开始，沿着内部结点的属性测试条件选择适当的分支，得到最终的叶结点分类。
categories: [机器学习,算法]
tags: [机器学习]
render_with_liquid: true
math: true
---

### 简介
决策树（Decision Tree）算法属于机器学习有监督分类算法的一种，决策树是一种判别模型。  
决策树是一种用于对实例进行分类的树形结构。由结点（node）和有向边（directed edge）组成。结点有两种类型：内部结点与叶结点，内结点表示一个特征或属性的测试条件（用于分开具有不同特性的记录），叶结点表示一个分类。  
用决策树模型，首先构建决策树，然后从决策树的根结点开始，沿着内部结点的属性测试条件选择适当的分支，得到最终的叶结点分类。

### 决策树基本流程



输入:   
训练集: $D={(x1,y1),(x2,y2),...,(xm,ym)}$  
属性集: $A={a1,a2,....,ad}$
过程：函数TreeGenerate(D,A)
```
   生成结点node
   if 
      D 中样本全属于同一类别C 
   then
      将node标记为C类叶结点; return;(1)
   end if
   if 
      A=空 OR D中样本在A上取值相同
   then
      将node标记为叶结点，其类别标记为D中样本数最多的类; return;(2)
   end if
   从A中选择最优划分属性a*;
   for a* 的每一个值 a*v do
      为node生成一个分支; 令Dv表示D中在a*上取值为a*v的样本子集;
      if Dv 为空 
      then
         将分支结点标记为叶结点，其类别标记为D中样本最多的类; return;(3)
      else
         以TreeGenerate(Dv,A\{a*})为分支结点
      end if
```

在决策树中有三种情况会导致递归返回：  
(1)当前节点包含的样本属于同一类别，不需要继续划分  
(2)当前属性集为空，或者所有样本在所有属性上取值相同，无法划分  
(3)当前节点包含的样本集为空，不能划分  
在(2)情形下，把当前节点标记为叶节点，并将其类别设定成该节点所含样本最多的类别；在(3)情形下，把当前节点标记为叶节点，并将其类别设定成其父节点所含样本最多的类别

### 划分选择
决策树学习的关键一步是选择最优划分属性。一般而言，随着划分过程不断进行。决策树的分支结点所包含的样本尽可能属于同一类别，即结点的“纯度”（purity）越来越高

#### 信息熵（Entropy）

给定节点t的信息熵公式为：  
$$Entropy (t)=-\sum_{j} P(j \mid t) \log P(j \mid t)$$  
其中 $P(j \mid t)$ 表示给定结点 $t$ 中属于类 $j$ 的记录所占比例。信息熵的值越小，则 $D$ 的纯度越高。

信息增益（Information gain），ID3决策树学习算法以信息增益为准则来选择划分属性。 

$$GAIN=Entropy (p)-\left[\sum_{i=1}^{k} \frac{n_{i}}{n}\right.Entropy \left.(p)\right]$$  

其中 p 为父结点，p 被划分为 k 类，$n_{i}$是划分为第 i 类的记录的数量。信息增益越大，则意味着使用属性a来进行划分所获得的“纯度提升”越大。

信息增益的缺点是信息增益大小是相对训练集而言的，训练数据集的经验熵比较大时，信息增益会偏大  
增益率（Information gain ratio），C4.5决策树学习算法以增益率为准则划分属性。  
$$SpiltInfo =-\sum_{i=1}^{k} \frac{n_{i}}{n} \log \frac{n_{i}}{n},$$  $$GainRatioSpilt=\frac{G A I N}{S p i l t I n f o}$$

信息增益准则对可取值数目较多的属性有所偏好，增益率准则对可取值数目较少的属性有所偏好，C4.5算法并不直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。

### 剪枝处理
剪枝是决策树学习算法中对付“过拟合”的主要手段。在决策树学习中，为了尽可能正确分类训练样本，结点划分过程将不断重复，有时会造成决策树分支过多，这时就可能因训练样本学的“太好”了，以致于把训练集自身的一些特点当作所有数据具有的一般性质而导致过拟合。因此，可通过主动去掉一些分支来降低过拟合的风险。  

决策树的剪枝的基本策略有“预剪枝”和“后剪枝”。预剪枝是指在决策树生成过程中，对每个结点在划分前进行估计，若当前结点的划分不能带来决策树泛化性能的提升，则停止划分并将当前结点标记为叶结点；后剪枝则是先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能的提升，则将该子树替换为叶结点。

