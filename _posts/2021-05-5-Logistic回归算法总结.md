---
title: Logistic回归算法总结
author: Mingxian Yang
date: 2023-08-19 18:10:00 +0800
description: 逻辑回归假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。
categories: [机器学习]
tags: [机器学习]
render_with_liquid: true
math: true
---

### 介绍
逻辑回归假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。

### 逻辑回归的假设

逻辑回归的第一个基本假设是假设数据服从伯努利分布。伯努利分布有一个简单的例子抛硬币，抛中为正面的概率是$p$，抛中为负面的概率是$1-p$。在逻辑回归这个模型里面是假设$h_{\theta}(x)$为样本为正的概率，$1-h_{\theta}(x)$为样本为负的概率。整个模型可以描述为
$$h_{\theta}(x;\theta)=p$$
逻辑回归的第二个假设样本为正的概率是：
$$
p=\frac{1}{1+e^{-\theta^{T} x}}
$$
所以逻辑回归的最终形式是：
$$
h_{\theta}(x;\theta)=\frac{1}{1+e^{-\theta^{T} x}}
$$

### 逻辑回归的损失函数
逻辑回归的损失函数是它的极大似然函数
$$
L_{\theta}(x)=\prod_{i=1}^{m} h_{\theta}\left(x^{i} ; \theta\right)^{y i} *\left(1-h_{\theta}\left(x^{i} ; \theta\right)\right)^{1-y^{i}}
$$

### 逻辑回归的求解方法
由于极大似然函数无法直接求解，因此需要通过对该函数进行梯度下降来不断逼近最优解。

这里的考点有批梯度下降、随机梯度下降、小批量梯度下降以及其他优化方式。

1. 批梯度下降，这种方式可以获得全局最优解，缺点是更新每个参数的时候需要遍历所有的数据，计算量太大，存在冗余数据，当数据量特别大的时候，每个参数的更新会很慢。

2. 随机梯度下降，这种方式每遍历一个样本更新一次参数，更新具有高方差。优点在于容易获得更好的局部最优解，但是收敛比较慢。

3. 小批量梯度下降，这种方法结合了批梯度下降和随机梯度下降的优点，每遍历一小批数据更新一次参数，减少了参数更新的次数，加快了收敛

上述三种方法还存在很多不足：

首先，是如何对模型选择合适的学习率。学习率保持不变不是一种好的选择。因为刚开始的时候，参数离最优解隔的比较远，需要保持一个较大的学习率尽快逼近最优解。但是，学习到后面的时候，参数和最优解隔得比较近，继续保持之前的学习率容易越过最优点。

其次，是如何对参数选择合适的学习率。在实践中，对每个参数保持同样的学习率是不合理的。有些参数更新频繁，那么学习率可以适当小一点。有些参数更新缓慢，那么学习率就应该大一点。

