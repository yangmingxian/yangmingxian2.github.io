---
title: 机器学习中的L1和L2正则化项
description: 机器学习中几乎都可以看到损失函数后面会添加一个额外项，常用的额外项一般有两种， L1正则化 和 L2正则化，或者 L1范数 和 L2范数。正则化之所以能够降低过拟合的原因在于，正则化是结构风险最小化的一种策略实现。给loss function加上正则化项，能使得新得到的优化目标函数h = f+normal，需要在f和normal中做一个权衡（trade-off），如果还像原来只优化f的情况下，那可能得到一组解比较复杂，使得正则项normal比较大，那么h就不是最优的，因此可以看出加正则项能让解更加简单，符合奥卡姆剃刀理论，同时也比较符合在偏差和方差（方差表示模型的复杂度）分析中，通过降低模型复杂度，得到更小的泛化误差，降低过拟合程度。
categories:
 - 机器学习
tags:
 - 机器学习
mathjax: true
---
### 概述
机器学习中几乎都可以看到损失函数后面会添加一个额外项，常用的额外项一般有两种， L1正则化 和 L2正则化，或者 L1范数 和 L2范数。
正则化之所以能够降低过拟合的原因在于，正则化是结构风险最小化的一种策略实现。

给loss function加上正则化项，能使得新得到的优化目标函数h = f+normal，需要在f和normal中做一个权衡（trade-off），如果还像原来只优化f的情况下，那可能得到一组解比较复杂，使得正则项normal比较大，那么h就不是最优的，因此可以看出加正则项能让解更加简单，符合奥卡姆剃刀理论，同时也比较符合在偏差和方差（方差表示模型的复杂度）分析中，通过降低模型复杂度，得到更小的泛化误差，降低过拟合程度。

L1正则化和L2正则化：

L1正则化就是在loss function后边所加正则项为L1范数，加上L1范数容易得到稀疏解（0比较多）。L2正则化就是loss function后边所加正则项为L2范数的平方，加上L2正则相比于L1正则来说，得到的解比较平滑（不是稀疏），但是同样能够保证解中接近于0（但不是等于0，所以相对平滑）的维度比较多，降低模型的复杂度。

### 什么是L1正则&L2正则

L1正则即将参数的绝对值之和加入到损失函数中，以二元线性回归为例，损失函数变为：  
$$
\min \frac{1}{2 m} \sum_{i=1}^{n}\left(h_{w}\left(x^{(i)}\right)-y^{(i)}\right)^{2}+\lambda \sum_{j=1}^{2}\left|w_{j}\right|
$$
L2正则即将参数的平方之和加入到损失函数中，以二元线性回归为例，损失函数变为：
$$
\min \frac{1}{2 m} \sum_{i=1}^{n}\left(h_{w}\left(x^{(i)}\right)-y^{(i)}\right)^{2}+\lambda \sum_{j=1}^{2} w_{j}^{2}
$$

### L1正则&L2正则的区别是什么
二者的区别的话，咱们总结主要有以下两点，最主要的还是第二点：

- L1正则化是指在损失函数中加入权值向量w的绝对值之和，即各个元素的绝对值之和，L2正则化指在损失函数中加入权值向量w的平方和。
- L1的功能是使权重稀疏，而L2的功能是使权重平滑。